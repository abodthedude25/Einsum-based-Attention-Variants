# Hardware-Aware Attention: Implementation and Cost Modeling in JAX

This repository provides from-scratch JAX implementations of various Transformer Attention mechanisms with a heavy focus on `einsum` notation.

**Objective:** This project serves a dual purpose:
1.  **To demonstrate clean, efficient, low-level implementation** of foundational LLM architectures using `einsum`.
2.  **To provide a first-principles performance model** that quantifies the primary trade-off in modern attention design: the reduction in memory bandwidth cost (KV cache size) versus computational cost (FLOPs).

This project directly analyzes the architectural benefits that address the memory bottleneck in LLM inference, a core challenge in efficient model deployment.

---

### Features & Analysis

This project implements and analyzes the following mechanisms:

1.  **Standard Multi-Head Attention (MHA):** The high-quality but memory-intensive baseline.
2.  **Grouped-Query Attention (GQA):** The industry-standard compromise for reducing KV cache pressure.
3.  **Multi-Value Attention (MVA):** A variant inspired by MatX research, decoupling K and V heads to optimize the quality/memory trade-off.

The analysis below is generated by the `main.py` script. It models the costs for a single forward pass with a batch size of 8, a context length of 4096, and a 7B parameter model profile (hidden_dim=4096, 32 heads).

```
Running Attention Mechanism Cost Analysis...
Config: Batch=8, SeqLen=4096, HiddenDim=4096, Heads=32

| Mechanism             | Compute (GFLOPs) | KV Cache Load (MB) | KV Cache Reduction |
| --------------------- | ---------------- | ------------------ | ------------------ |
| MHA (32H)             | 2199.0           | 512.0              | 1.0x (Baseline)    |
| GQA (32Q, 8KV)        | 2199.0           | 128.0              | **4.0x** |
| MVA (32Q, 1K, 32V)    | 2199.0           | 264.0              | **1.9x** |

--- Testing JAX Implementations ---
MHA Output Shape: (4, 64, 256)
GQA Output Shape: (4, 64, 256)
MVA Output Shape: (4, 64, 256)
```

**Key Insight:** The model demonstrates that GQA drastically cuts memory traffic by 4x with identical computational cost. MVA offers a different trade-off point, nearly halving the memory cost of MHA while preserving the full representational capacity of 32 value heads, unlike GQA which reduces it to 8.

---

### `einsum` Walkthrough: Multi-Head Attention

The core of MHA can be expressed in two `einsum` operations. Let's assume the following dimensions:
- `B`: Batch size
- `H`: Number of heads
- `S`: Sequence length (for queries)
- `K`: Key/Value sequence length
- `D`: Dimension of each head

**1. Calculate Attention Scores:**
```python
# q shape: (B, H, S, D)
# k shape: (B, H, K, D)
scores = jnp.einsum('BHSD,BHKD->BHSK', q, k)
```
This performs a batched dot product between each query head and each key head, resulting in an attention matrix of shape `(B, H, S, K)`.

**2. Apply Scores to Values:**
```python
# probs shape: (B, H, S, K) - (scores after softmax)
# v shape: (B, H, K, D)
output = jnp.einsum('BHSK,BHKD->BHSD', probs, v)
```
This uses the attention probabilities to create a weighted sum of the value vectors for each query, resulting in the final output of shape `(B, H, S, D)`.

---

### Usage

1.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
2.  **Run the example:**
    ```bash
    python main.py
    ```